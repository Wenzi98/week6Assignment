{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd0f675-bd45-4377-be98-aed56ea9f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:45:31.155811: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-10 12:45:31.573320: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-10 12:45:31.923486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752151532.212922     188 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752151532.305646     188 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752151532.977314     188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752151532.977344     188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752151532.977346     188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752151532.977348     188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 12:45:33.038008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:45:39.182922: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/167514a9-0b28-4d9a-b9ac-f727135ffe83/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - accuracy: 0.2496 - loss: 1.4177 - val_accuracy: 0.2450 - val_loss: 1.3867\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.2774 - loss: 1.3867 - val_accuracy: 0.2450 - val_loss: 1.3857\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.2837 - loss: 1.3831 - val_accuracy: 0.3200 - val_loss: 1.3842\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.2448 - loss: 1.3844 - val_accuracy: 0.3000 - val_loss: 1.3870\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.2674 - loss: 1.3874 - val_accuracy: 0.2850 - val_loss: 1.3846\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.2679 - loss: 1.3799 - val_accuracy: 0.3200 - val_loss: 1.3707\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.2689 - loss: 1.3841 - val_accuracy: 0.3200 - val_loss: 1.3802\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.2625 - loss: 1.3791 - val_accuracy: 0.2400 - val_loss: 1.3853\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.2890 - loss: 1.3698 - val_accuracy: 0.3200 - val_loss: 1.3771\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.3676 - loss: 1.3523 - val_accuracy: 0.1700 - val_loss: 1.4475\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1624 - loss: 1.4666\n",
      "\n",
      "Test accuracy: 0.17\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpy3xgd01g/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy3xgd01g/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpy3xgd01g'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  128764266731952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764266727376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764266737936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764266736704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764266734768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764267376560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764267375152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764267377616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764267379024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128764267367760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1752151572.331500     188 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1752151572.331529     188 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TFLite Prediction: metal (Confidence: 30.32%)\n",
      "\n",
      "==================================================\n",
      "Edge AI Deployment Report\n",
      "==================================================\n",
      "\n",
      "Model Metrics:\n",
      "- Test Accuracy: 17.00%\n",
      "- Model Size: 209.28 KB\n",
      "\n",
      "Deployment Steps:\n",
      "1. Train model with edge-appropriate architecture\n",
      "2. Quantize model using TFLiteConverter optimizations\n",
      "3. Transfer .tflite file to Raspberry Pi storage\n",
      "4. Use TFLite Interpreter API in Python/C++ application\n",
      "5. Process camera input through model in real-time\n",
      "\n",
      "Edge AI Benefits for Real-time Applications:\n",
      "- ⚡ Low Latency: Local processing eliminates network round-trips\n",
      "- 🔒 Privacy: Sensitive image data never leaves the device\n",
      "- 📴 Offline Operation: Functions without internet connectivity\n",
      "- 💰 Cost Efficiency: Reduces cloud processing costs\n",
      "- 🔋 Energy Savings: Lower power consumption than cloud transmission\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:46:12.332160: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpy3xgd01g\n",
      "2025-07-10 12:46:12.333243: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-07-10 12:46:12.333255: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpy3xgd01g\n",
      "I0000 00:00:1752151572.338501     188 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-07-10 12:46:12.339522: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-07-10 12:46:12.374966: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpy3xgd01g\n",
      "2025-07-10 12:46:12.386012: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 53857 microseconds.\n",
      "2025-07-10 12:46:12.486512: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "/home/167514a9-0b28-4d9a-b9ac-f727135ffe83/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ---------------------\n",
    "# 1. Data Preparation\n",
    "# ---------------------\n",
    "# Simulating a recyclable items dataset (paper, glass, plastic, metal)\n",
    "# In real deployment, use actual images from edge device cameras\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = ['paper', 'glass', 'plastic', 'metal']\n",
    "\n",
    "# Generate synthetic dataset (replace with real images in production)\n",
    "def generate_synthetic_data(num_samples=1000):\n",
    "    images = np.random.randint(0, 256, (num_samples, 96, 96, 3), dtype=np.uint8)\n",
    "    labels = np.random.randint(0, NUM_CLASSES, num_samples)\n",
    "    return images, labels\n",
    "\n",
    "train_images, train_labels = generate_synthetic_data(800)\n",
    "test_images, test_labels = generate_synthetic_data(200)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def preprocess(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    images = tf.image.resize(images, (96, 96))\n",
    "    return images, labels\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(100).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset = test_dataset.map(preprocess).batch(16)\n",
    "\n",
    "# ---------------------\n",
    "# 2. Model Training\n",
    "# ---------------------\n",
    "# Lightweight CNN suitable for edge devices\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(8, (3, 3), activation='relu', input_shape=(96, 96, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(NUM_CLASSES)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model (simplified for demo - use more epochs with real data)\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
    "\n",
    "# Evaluate final accuracy\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'\\nTest accuracy: {test_acc:.2f}')\n",
    "\n",
    "# ---------------------\n",
    "# 3. TFLite Conversion\n",
    "# ---------------------\n",
    "# Convert to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantization for size reduction\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save converted model\n",
    "with open('recyclable_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# ---------------------\n",
    "# 4. TFLite Inference\n",
    "# ---------------------\n",
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input/output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test on sample image\n",
    "sample_image, _ = preprocess(test_images[0:1], test_labels[0:1])\n",
    "interpreter.set_tensor(input_details[0]['index'], sample_image)\n",
    "interpreter.invoke()\n",
    "tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Display results\n",
    "prediction = np.argmax(tflite_output[0])\n",
    "confidence = tf.nn.softmax(tflite_output[0])[prediction]\n",
    "print(f\"\\nTFLite Prediction: {CLASS_NAMES[prediction]} \"\n",
    "      f\"(Confidence: {confidence:.2%})\")\n",
    "\n",
    "# ---------------------\n",
    "# 5. Deployment Report\n",
    "# ---------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Edge AI Deployment Report\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nModel Metrics:\")\n",
    "print(f\"- Test Accuracy: {test_acc:.2%}\")\n",
    "print(f\"- Model Size: {len(tflite_model)/1024:.2f} KB\")\n",
    "\n",
    "print(\"\\nDeployment Steps:\")\n",
    "print(\"1. Train model with edge-appropriate architecture\")\n",
    "print(\"2. Quantize model using TFLiteConverter optimizations\")\n",
    "print(\"3. Transfer .tflite file to Raspberry Pi storage\")\n",
    "print(\"4. Use TFLite Interpreter API in Python/C++ application\")\n",
    "print(\"5. Process camera input through model in real-time\")\n",
    "\n",
    "print(\"\\nEdge AI Benefits for Real-time Applications:\")\n",
    "print(\"-  Low Latency: Local processing eliminates network round-trips\")\n",
    "print(\"-  Privacy: Sensitive image data never leaves the device\")\n",
    "print(\"-  Offline Operation: Functions without internet connectivity\")\n",
    "print(\"-  Cost Efficiency: Reduces cloud processing costs\")\n",
    "print(\"-  Energy Savings: Lower power consumption than cloud transmission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507f51d-0ac8-4033-b30e-08d9cd6f125a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
